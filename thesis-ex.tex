%%
%% This is file `thesis-ex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% uiucthesis.dtx  (with options: `example')
%% 
\def\fileversion{v2.25} \def\filedate{2023/01/30}
%% Package and Class "uiucthesis" for use with LaTeX2e.
\documentclass[draftthesis,fullpage]{uiucthesis}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{etoolbox}
\begin{document}

\title{Robust Three-dimensional Particle Tracking for\\ Small-scale Dynamics}
\author{Liu Hong\\[1cm]{\small Advisor: Leonardo P. Chamorro}}
%\author{Liu Hong}
%\author{Leonardo P. Chamorro}
\department{Mechanical Science and Engineering}
\phdthesis
\advisor{Leonardo P. Chamorro}

\maketitle

\frontmatter

%% Create an abstract that can also be used for the ProQuest abstract.
%% Note that ProQuest truncates their abstracts at 350 words.
%\begin{abstract}
%3D particle tracking is a powerful technique to understand particle %dynamics and complex fluid flow. 
%\end{abstract}

%% Create a dedication in italics with no heading, centered vertically
%% on the page.


%% Create an Acknowledgements page, many departments require you to
%% include funding support in this.
%\chapter*{Acknowledgments}

%This project would not have been possible without the support of
%many people. Many thanks to my adviser, Leonardo P. Chamorro, who
%trained me from a boy lost in research to an independent researcher.
%Without his support and guidance, I can't make so far along the way of %this study. 


%% The thesis format requires the Table of Contents to come
%% before any other major sections, all of these sections after
%% the Table of Contents must be listed therein (i.e., use \chapter,
%% not \chapter*).  Common sections to have between the Table of
%% Contents and the main text are:
%%
%% List of Tables
%% List of Figures
%% List Symbols and/or Abbreviations
%% etc.

\tableofcontents
%\listoftables
%\listoffigures

%% Create a List of Abbreviations. The left column
%% is 1 inch wide and left-justified
%\chapter{List of Abbreviations}

%\begin{symbollist*}
%\item[CA] Caffeine Addict.
%\item[CD] Coffee Drinker.
%\end{symbollist*}

%% Create a List of Symbols. The left column
%% is 0.7 inch wide and centered
\chapter{Introduction}\label{chapter:intro}

Significant advancements in both software and hardware are necessary to develop a technique capable of real-time tracking of particles in small volumes. Such a technique would provide a valuable opportunity to investigate particle dynamics and complex fluid flows. When tracking particles in volumes of approximately 100 mm, the tomographic approach is a popular method due to its high spatial resolution and flexibility in the investigation volume. This approach involves four steps: particle reconstruction, track initialization, prediction, and optimization. While previous researchers have proposed advanced prediction and optimization methods such as iterative particle reconstruction  (IPR) \citep{wieneke2012iterative} and Shake-The-Box \citep{schanz2016shake}, the presence of numerous ghost particles created during triangulation limits their applicability in industrial settings. To address this issue, this study draws inspiration from optimizing the four-view triangulation task discussed in the \hyperref[chapter:DLT]{Parallelized DLT} chapter and focuses on small-scale particle tracking.

In recent decades, microfabrication techniques have made significant progress, offering new possibilities for microfluidics, particularly in the design of biomedical devices and the control of various processes such as cooling \citep{singhal2004microscale}, mixing  \citep{song2003millisecond,liu2004two}, extraction \citep{hisamoto2001chip, kluge2009production} and drug delivery \citep{tsai2007review, nguyen2013design}. This has led to an increased demand for tracking particles in small volumes. To overcome constraints imposed by illumination and space, various techniques based on standard particle tracking methods have been developed, including holographic \citep{sheng2006digital, soria2008towards, memmolo2011twin, daloglu2018label}, confocal \citep{kinoshita2007three, lima2008vitro}, stereoscopic \citep{lindken2006stereoscopic, bown2006three}, and defocus-based \citep{yoon20063d, pereira2007microscale} variants. Further discussion on these techniques can be found in \citet{wereley2010recent}. The development of microlens manufacturing technology and camera sensing has created new possibilities for utilizing the so-called light field, LF \citep{gershun1939light}, which describes the distribution of light rays in space instead of 2D projection, in plenoptic function models \citep{adelson1991plenoptic}. These techniques offer distinct possibilities for particle tracking in small volumes.  

The use of light field cameras has been documented in various areas, such as photography \citep{levoy1996light, ng2005fourier}, computer vision \citep{tao2013depth, adelson1992single} and robotic navigation \citep{kaveti2020light}. \citet{belden2010three} demonstrated the application of LF imaging concepts in a large camera array to capture multiple views simultaneously. In contrast to tomo-PIV, they implemented synthetic aperture refocusing, so-called synthetic aperture particle image velocimetry (SA-PIV), using a map-shift-average algorithm. However, the requirement for many cameras could result in a bulky and costly system.

Utilizing a microlens array (MLA) in the optical system of a conventional microscope rather than employing a multi-camera approach has expanded the range of light field (LF) applications to the micro scale, all with the aid of a single, high-resolution camera \citep{levoy2006light}. Compared to SA-PIV or tomo-PIV, LF presents several advantages. The camera spatial calibration, which can be challenging in microscopic measurements, is unnecessary when an MLA is pre-calibrated with a CCD or CMOS sensor. The relative independence of the primary optic system allows LF to be widely applied to various microscopes and macro lenses, producing robust 3D data. Thus, single-camera LF micro particle velocimetry (LF-PIV), proposed by \cite{lynch2012three}, has become a popular complementary approach for space-constrained measurements \citep{shi2016dense, shi2016parametric, shi2017light, fahringer2015volumetric, xu20173d, li2017investigation}. Although there are other single-camera methods, such as digital holography, high-power laser requirements \citep{hinsch2002holographic} and low signal-to-noise ratios \citep{ooms2009digital} may limit their applications, as well as their lengthy reconstruction time.














Notwithstanding, there exist several obstacles that impede the capacity and progress of LF-PIV. One of the primary drawbacks pertains to the substantial computational expense associated with 3D reconstruction. Similar to traditional tomo-PIV, the spatial domain is discretized into cubical voxels, and the optic system projects $E(x,y,z)$ onto a 2D pixel-wise intensity distribution $I(x,y)$ \citep{fahringer2015volumetric}, which can be expressed as:
\begin{equation}
  \sum_{j \in N_{i}} w_{i,j} E\left(x_j, y_j, z_j\right)=I\left(x_i, y_i\right),
  \label{LF_principle}
\end{equation}
where $N_i$ indicates the number of voxels in the line-of-sight of the $i-$th pixel, and $w_{i,j}$ represents a weighting coefficient dependent on the optic system. The problem encountered is ill-posed due to an under-determined system of equations. To address this issue, a series of algorithms known as algebraic methods have been developed to solve the linear equation system iteratively. These methods were first introduced by \cite{herman1976iterative}. One such algorithm, the standard multiplicative algebraic reconstruction technique (MART), proposed by \cite{belden2010three}, is widely used with suitable modifications to compute the intensity of each voxel:
\begin{equation}
       E\left(x_j,y_j,z_j\right)^{k+1}=E\left(x_j,y_j,z_j\right)^{k}\left(\frac{I\left(x_i, y_i\right)}{\sum_{j \in N_j} w_{i,j} E\left(x_j, y_j, z_j\right)^k}\right)^{\mu w_{i, j}}.
       \label{LF_MART}
\end{equation}

Here, $k$ is the iteration number, and $\mu$ is a relaxation factor. The reconstruction process in LF-PIV encounters challenges due to the high computational cost associated with the weight matrix calculation and iterative steps, which arise from the fact that many pixels can capture one voxel. The problem is further compounded by the substantial amount of data required to store the non-zero values of the weight matrix, which can easily exceed 350 GB for a $300 \times 200 \times 200$ voxel and $850 \times 850$ pixel system. This leads to extended processing times, even for low-resolution images, with reconstructions taking up to an hour on a 12-core workstation with a RAID 0 solid-state disk array. While methods such as dense ray tracing-based MART \citep{shi2017light} and pre-recognition simultaneous algebraic reconstruction technique \citep{zhu2021pre} have been proposed to improve the efficiency of reconstruction, the issue of the weight matrix bottleneck remains significant.

To address the sparse particle concentration characteristic for small-scale particle tracking and the orthographic view of the microscope, \citet{levoy2006light} and \citet{truscott2017three} utilized a 3D deconvolution algorithm to remove blurring effects in the images focal stack. While 3D deconvolution is more efficient than algebraic methods as it avoids storing large weighting matrices, iterative deconvolution, such as the Richardson-Lucy algorithm \citep{sibarita2005deconvolution}, requires significant computational power to store a focal stack of images and perform voxel-wise deconvolution. Estimating the point-spread-function (PSF), which is necessary for deconvolution, requires the absence of noise and aberrations, which can be achieved by empirically refocusing a sub-resolution fluorescent particle at the center of a lenslet \citep{levoy2006light}. However, it may not be feasible to image sub-resolution fluorescent particles with the same optical system used in a PIV setup, leading to inappropriate PSF estimation. Additionally, the PSF is not invariant to translation, making it unsuitable to use one PSF to deconvolve all voxels \citep{zhu2021pre}. Despite the potential performance of the algorithm, alignment between pixels of sub-images and MLA, which ensures a consistent square matrix of pixels underneath each lenslet, and the MLA rotated to axis-align with the camera sensor, is challenging to achieve during calibration due to lens distortion and the fall of intensity towards the periphery of an image \citep{piller2012microscope}.

This study expands the utilization of non-iterative ray tracing 3D microscale particle tracking, as described in the \hyperref[chapter:MPT]{Microscale Particle Tracking} chapter, to larger \hyperref[chapter:cm]{cm-scale 3D Particle Tracking} systems, which present more intricate fluid field and particle motion dynamics more challenging to simulate than their microscale counterparts. \hyperref[chapter:cm-scale]{Cm-scale 3D Particle Tracking} chapter outlines the primary difficulties encountered when employing this methodology with a perspective view system.








\renewcommand{\cleardoublepage}{} %TODO remove those two lines if not reach 20 page
\renewcommand{\clearpage}{}
\chapter{Parallelizing DLT for Improved Performance}\label{chapter:DLT}
In this study, a a 4-camera system was used to capture 3D trajectories of grains inside a harvester to optimize the shelling and separation process. An intrinsic and extrinsic calibration method based on the Charuco board was employed to simplify the calibration procedure and enhance its robustness, as previously proposed by \cite{an2018charuco}. To eliminate ghost particles and accurately compute the 3D position of particle candidates, a parallelized direct linear transformation (DLT) approach was employed. This algebraic 4 ray intersection method has since inspired subsequent research into $N$ rays intersection approaches.



\subsection*{Experiment Setup and 3D Tracking Principles}

A set of four a2A1920-160umBAS cameras were installed to capture image sequences of the investigation volume from different angles. The camera system was calibrated by capturing 1000 frames of a charco board with a grid size of 7 $\times$ 10 and a dimension of 20 mm, whose position was changed systematically. Initially, each camera's intrinsic parameter $K$ and distortion coefficients $d$ were calibrated from their corresponding image sequences.

Utilizing 2D-2D correspondences of unique charco board corners, the relative pose between each pair of cameras, denoted as $X_j = R_{ij}X_i + T_{ij}$, where $R_{ij}$ represents a rotation matrix and $T_{ij}$ denotes a translation vector, can be estimated for a 4-camera system. By defining the first camera coordinate system as the real-world coordinate system with $R_1 = I_{3 \times 3}$ and $T_1 = (0,0,0)^T$, the poses of all four cameras can be registered in the real-world coordinate system. A camera graph $G$ is constructed with minimum degree $\delta (G) \geq 2$, which is then fine-tuned using bundle adjustment as described by \cite{engels2006bundle}. Once the 4-camera system was calibrated, synchronized image sequences were captured at a rate of 150 fps as beans were released above the investigation volume.

The calibrated 4-camera system used in this study is depicted in figure \ref{fig:DLT}. Each bean with a real-world coordinate of $(x,y,z)$ was captured by four cameras, resulting in corresponding image coordinates of $(u_1,v_1)$, $(u_2,v_2)$, and so on. As per the pinhole camera model, the relationship between particle image coordinates and real-world coordinates in a homogeneous coordinate system is determined by the following equation:
\begin{equation}
       \left[\begin{array}{c}
              u_i \\
              v _i\\
              1
              \end{array}\right]=K_i\left[\begin{array}{ll}
              R_i & T_i
              \end{array}\right]\left[\begin{array}{c}
              x \\
              y \\
              z \\
              1
              \end{array}\right] = \left[\begin{array}{c}
                     \overrightarrow{P_{i,1}}\\
                     \overrightarrow{P_{i,2}}\\
                     \overrightarrow{P_{i,3}}
              \end{array}\right] \left[\begin{array}{c}
                     x \\
                     y \\
                     z \\
                     1
                     \end{array}\right]      , i = 1,2,3,4,
\end{equation}
where $\overrightarrow{P}_{i,i=j}$ is $j^{th}$ row vector of projection matrix \textbf{$P_i$}. Based on DLT algorithm \citep{shapiro1978direct}, real particle position vector $\overrightarrow{X}$ can be solved by minimizing $\vec{w}$ :
\begin{equation}
       A \vec{X}=\left[\begin{array}{c}
       v_1 \vec{p}_{1,3}-\vec{p}_{1,2} \\
       \vec{p}_{1,1}-u_1 \vec{p}_{1,3} \\
       v_2 \vec{p}_{2,3}-\vec{p}_{2,2} \\
       \vec{p}_{2,1}-u_2 \vec{p}_{2,3} \\
       \vdots
       \end{array}\right] = \vec{w} \approx 0,
\end{equation}
where $\vec{w}$ is residue induced by noise. With SVD decomposition of $A$,  $\overrightarrow{X}$ is the corresponding column vector of $V^T$ with the smallest value of $s$. 
\begin{figure}[h]
       \centerline{\includegraphics[width=0.6\linewidth]{fig/DLT.eps}}
       \caption{4 view 3D particle reconstruction.}
       \label{fig:DLT}
\end{figure}
As a consequence of weak matching between views for 2D particle detection, the computation of $\overrightarrow{X}$ candidates and corresponding $s$ was performed using the Direct Linear Transform (DLT) method, augmented with optimized Single Instruction Multiple Data (SIMD) instructions, implemented through OpenMP. The subsequent removal of ghost particle positions was achieved by sorting $s$ in descending order. Additionally, the methodology outlined by \cite{crocker1996methods} for particle tracking was employed to further eliminate any outliers.








\subsection*{Results and Remarks}
We achieved almost instantaneous 4-view 3D particle tracking across a vast spatial domain by eliminating most ghost particles during triangulation. The fundamental concept of reconstructing particles using $N$ ($N = 4$) rays has catalyzed a subsequent, more modestly scoped particle tracking investigation.


\chapter{Microscale Particle Tracking - Prototype}\label{chapter:MPT}
\vspace{-15mm}
This Chapter is adapted from: Hong, L. and  Chamorro L.P. "A fast, non-iterative ray-intersection approach for three-dimensional microscale particle tracking." \textbf{Lab on a Chip}, 22.5: 964-971, 2022.
\\

In this work, we propose a non-iterative ray tracing method with robust post-capture microlens array sensor alignment to reconstruct sparse particle concentration in light field particle image velocimetry and particle tracking velocimetry nearly instantaneously. Voxels traversed by various rays are stored by kd-tree to reduce memory load and computational time. Cloud point classification algorithm is employed for particle identification and spatial reconstruction. The approach is tested with a physically-based realistic model of a light field camera. Also, an optical system is assembled in a microscope to directly obtain the 3D laminar velocity field in the fully-developed region, which exhibits good agreement with the theoretical solution.

\subsection*{Light Field Microscopy}
Consider placing a microlens array in the intermediate image plane between a tube lens of an infinity-corrected microscope and a camera sensor to enable the microscope to capture light field in a single photograph \citep{levoy2006light}; see a basic diagram in figure \ref{fig:lf_principle}.

\begin{figure}[h]
       \centerline{\includegraphics[width=1\linewidth]{fig/LFM.eps}}
         \caption{Schematics of a Light field system with an infinity-corrected microscope. Red lines indicate rays from in focus particle, whereas green lines denote those for an out-of-focus particle.}
       \label{fig:lf_principle}
\end{figure}

A 4D array $L(u,v,s,t) $ \citep{levoy1996light} describing the intersection of rays between the sensor ($u,v$) and MLA ($s,t$) planes can be converted from a raw light field image.  The sensor plane is located at the focal plane of MLA with a distance equal to the focal length of the lenslet $f_l$. The $(u,v)$ coordinates are associated with pixels in each lenslet, encoding angular information. The $(s,t)$ coordinates are associated with the center of each lenslet and encode spatial information. In each sub-image in $(u,v)$ coordinates, the lenslet center is identified as the origin (0,0).

Due to it is not practical to perfectly align the lenslet center with a pixel in the camera sensor, pixels in each lenslet are interpolated onto a uniform $(u,v)$ grid for computational efficiency. By decoding the 4D array $L(u,v,s,t)$, depth information can be recovered at the expense of spatial resolution \citep{levoy2006light}. For this 3D reconstruction approach, the spatial resolution is modulated by the lenslet size $d_m$, whereas the depth resolution is governed by the sub-image resolution $N_s = d_m / d_p$, where $d_p$ is the pixel size.  Once 2D raw light field images are reshaped to a 4D array, 3D intensity distribution can be calculated using algebraic or 3D convolution methods. 

Since the majority of pixels do not receive rays from the particles and PIV/PTV only needs to reconstruct the location of the particles, rays unit vector pre-filtered data matrix $Q$ (\autoref{unit_vector}) and rays counter matrix $C(x_r,y_r,z_r)$ are introduced to reduce the computational load. A simple intensity threshold filter computed based on raw images without particles 
may be used to binarize light field images to select pixels lit by particles to form
\begin{equation}
       Q=\left[\begin{array}{ccccc}
       \mid & \mid & \mid & \mid & \mid \\
       x_p & y_p & \hat{i} & \hat{j} & \hat{k} \\
       \mid & \mid & \mid & \mid & \mid
       \end{array}\right], \quad Q \in C^{M \times 5}
       \label{unit_vector}
\end{equation}
where $M$ indicates the number of rays used for ray tracing, $(x_p, y_p)$ is the pixel coordinates on the sensor and $(\hat{i}, \hat{j}, \hat{k})$ is the unit direction vector given by connecting the pixel and its corresponding lenslet center treated as a pinhole.

Then, the 3D reconstruction of particles can be converted to an "intersection points" finding problem (figure \ref{fig:ray_counter}a) of 3D skew lines (Rays). The refocused image volume is discretized into small voxels, and the ray counter matrix $C(x_r,y_r,z_r)$ is zero-initialized to solve this problem. Since all rays are nearly perpendicular to the ($x,y$) plane, we consider that a ray can be conceptualized as passing through the voxel if it intersects with the center plane of the voxel (figure \ref{fig:ray_counter}b) to accelerate calculation. The spatial coordinates of intersected points for rays with different depth center planes can be computed as follows:
\begin{equation}
  \left[\begin{array}{c}
    {x_p}^{\prime} \\
    {y_p}^{\prime}
    \end{array}\right]=\frac{z_r}{\hat{k}}\left[\begin{array}{c}
    \hat{i} \\
    \hat{j}
    \end{array}\right] + 
    \left[\begin{array}{c}
    x_p \\
    y_p
    \end{array}\right]
  \label{eq:intersection}
\end{equation}

Using a binary search, the rays counter of the closest voxel $(x_r,y_r,z_r)$ of the intersection point $(x_p’, y_p’, z_r)$ is incremented by one. The counting is performed with atomic operation in GPU's shared memory to reduce processing time  (figure \ref{fig:ray_counter}b). The ray counter matrix $C(x_r,y_r,z_r)$ is filtered by half of the number of pixels lit by a particle in-focus plane to avoid noncritical cloud point processing; it is computed as
\begin{equation}
    C_{min} =  \lfloor \frac{1}{2}(R_p \times M / q)^2 \pi \rfloor,
  \label{eq:RayCounterThreshold}
\end{equation}
where $\lfloor x \rfloor$ is the floor function,  $M$ is the magnification of the objective lens, $R_p$ is the radius of the particle, and $q$ is the pixel size. Then, according to the particle density in the discretized refocused image volume, sparse voxels with ray counters can be classified and further reduced to a one-to-one correspondence between particles and voxels. If points clouds are relatively dense, voxels with local maximum ray counters are screened with a fast nearest neighbor search of kd tree \citep{ramasubramanian1989generalized}. Otherwise, density-based spatial clustering of application with noise, DBSCAN \citep{ester1996density} is utilized to classify voxels, and the center positions are computed via averaging throughout classes. See the illustrative flowchart in figure \ref{fig:flowchart}.

\begin{figure}[h]
       \centerline{\includegraphics[width=0.7\linewidth]{fig/figure2.eps}} 
       \caption{(a) 3D light ray diagram of an image volume around the microlens array, MLA; (b) increment of ray counter in one voxel.}
       \label{fig:ray_counter}
\end{figure}
       
       
\begin{figure}[h]
       \centerline{\includegraphics[width=0.7\linewidth]{fig/figure3.eps}}
       \caption{(a) Cloud point classification and center position calculation flowchart; (b) 3D cloud point for particle center position calculation.}
       \label{fig:flowchart}
\end{figure}

\section*{Ray simulation}\label{sec:simulation}
Now, we introduce a physically-based simulation \citep{michels2018simulation} with a realistic lens to compare the ground truth with a refocused position of particles. An $f/2.0\pm 22^{\circ}$ double-Gaussian camera lens is constructed in Blender, as shown in figure \ref{fig:simulate_lens}a. The objective lens's surfaces are discretized into 64 radial and 128 longitudinal vertices to achieve an approximately round surface, and a material shader is used for normal surface correction. Details of this particular lens can be found in \citet{smith2005modern}. With 2D Ray simulation in Zemax, the optic system can be treated as a thick convex lens with two cardinal planes $H_1$, $H_2$ as illustrated in figure \ref{fig:simulate_lens}b. 
\begin{figure*}[h]
       \centerline{\includegraphics[width = 0.8\linewidth]{fig/figure4.eps}} 
        \caption{(a) Schematics of the double-Gaussian camera model with MLA;  (b) an equivalent thick convex lens with two cardinal planes, $H_1$ and $H_2$.}
      \label{fig:simulate_lens}
\end{figure*}
\begin{figure*}[h]
       \centerline{\includegraphics[width=0.8\linewidth]{fig/figure5.jpg}}
       \caption{Configuration of LF simulation for uncertainty analysis of 3D particle reconstruction. (a) Three layers of particles in the objective space; (b) corresponding LF images; (c) reconstruction with ground truth; (d) Simulation details.}
       \label{fig:simu}
\end{figure*}
The scattered light from an object in position $A$ has a corresponding image position $A^{\prime}$ calculated as follows:
\begin{equation}
  \frac{1}{EFL} = \frac{1}{s^{\prime}} + \frac{1}{s}
\label{eq:z_image}
\end{equation}
\begin{equation}
  \left[\begin{array}{c}
    {x_i} \\
    {y_i}
    \end{array}\right]=\frac{s^{\prime}}{s}\left[\begin{array}{c}
    x \\
    y
    \end{array}\right] 
  \label{eq:xy_image}
\end{equation}
where EFL is the effective focal length. All other parameters used in this simulation are based on the experiment's device to compare with a real experimental setup. A rectangular MLA with pitch $p_m$ = 125 $\mu$m and focal length $f_m$ = 3.75 mm is placed at 2$EFL$ behind the $H_2$ plane. The curvature radius $R_1$ of each lenslet of the MLA with a negligible thickness of backplate, $d \approx 0$, and flat backplane, $R_2 = \infty$, is computed based on the lens maker's equation
\begin{equation}
  \frac{1}{f_m}=(n-1)\left(\frac{1}{R_{1}}-\frac{1}{R_{2}}+\frac{(n-1) d}{n R_{1} R_{2}}\right)=\frac{n-1}{R_{1}}.
\label{eq:lensmaker}
\end{equation}
Here, $n = 1.56$ is the index of refraction, IOR, according to the MLA material (MLA-S125-f30) used in the experiment. A $35.9$ mm $\times$ $23.9$ mm camera sensor with 8256 pixels $\times$ 5504 pixels (Nikon D850), is placed on the focal plane of the MLA as shown in figure \ref{fig:simu}d.

Spherical particles of radius $r=30$ $\mu$m are placed at various depths $z$, namely, $d_0$, $d_1$,  and $d_2$ in figure \ref{fig:simu}d) with a distance of around 2$EFL$ from the $H_1$ plane. Based on equation \ref{eq:z_image}, the in-focused particle image is located in the MLA plane. Emission shade is added to the particle's surface to simulate light reflection or emission in a real experiment. By using implemented cycles render engine, 2048 render samples per pixel are computed with GPU. Three raw LF images correspond to 15 particles on the $d_0$, $d_1$ and $d_2$ planes. Figure \ref{fig:simu}a shows the particle positions in the objective volume, and figure \ref{fig:simu}b shows the raw counterpart. Then, a calibration image is rendered by creating an area light source on $d_0$ and reducing the aperture size of the main lens to a sufficiently small value. 

The image volume is discretized into 125 $ \mu$m ($x$) $\times$ 125 $\mu$m ($y$) $\times$ 250 $\mu$m ($z$) voxels and the corresponding ray counter matrix $C(x_r,y_r,z_r)$ is initialized. Finally, the 3D particle positions are reconstructed to voxels in the image volume and compared with theoretical image positions calculated from equation \ref{eq:z_image} and \ref{eq:xy_image}; see figure \ref{fig:simu}c. The mean error distance in image volume is 1.24 mm, caused by lateral resolution, limited angular resolution, radial distortion, depth distortion, vignetting, and the limited number of simulated rays. However, high magnification and tele-centricity (orthographic views) of the macro lens and microscopes can reduce distortion induced by the lens. All rays that go through MLA are recorded by pixels in the real experiment. The distance error in object volume should be recomputed based on the lens magnification, $M$, i.e., 
\begin{equation}
  \Delta x = \frac{\Delta x_i}{M}, \Delta y = \frac{\Delta y_i}{M}, \Delta z = \frac{\Delta z_i}{M^2}
\label{eq:error}
\end{equation}
where $\Delta x_i$, $\Delta y_i$, and $\Delta z_i$ are distance errors in the image volume, whereas $\Delta x$, $\Delta y$, and $\Delta z$ are distance errors in the object volume. For instance, with a 20X objective lens, 1.24 mm distance error under a large lens distortion leads to an only upper bound of 10.6 $\mu$m distance error (around 10$R_p$) in the objective volume. The distance error of particle 3D reconstruction of the microscope is much smaller compared to the simulation case.

\section*{Experiment - Proof of concept}
A 3D PIV experiment is conducted in a rectangular microchannel of 200 $\mu$m $\times$ 200 $\mu$m cross-section and $\times$ 58.5 mm long; see basic schematics  in figure \ref{fig:setup}a. The system uses a Nikon Eclipse Ti2 inverted microscope, which utilizes fluorescence filter cubes to capture high signal-to-noise ratio images. A Nikon CFI Plan Fluor 20X/0.5 objective lens provides high contrast fluorescence observation and a 45.4 megapixel Nikon D850 camera (pixel size $p=4.35$ $\mu$m) with nose-to-nose 50 mm f/1.8D lenses are employed to capture 1:1 images with a large aperture. A 25.4 mm round MLA (RPC photonics S125-f30) contains lens lets with pitch size $p_l$ = 125 $\mu$m and focal length $f_m$ = 3.75 mm. 
A continuous 532 nm laser is used to illuminate the interrogation volume through a dichroic filter cube and objective lens. The seeding consists of 1.7-2.2 $\mu$m fluorescent Nile Red particles (Spherotech), whose emission spectral peak is around 560 nm. A high-pass filter in the cube blocks the reflections of the laser light and allows light emission from particles captured from the camera. A programmable Harvard syringe pump pumps a distilled water-particle solution to the microchannel through microtubes at a flow rate of  0.85 $\mu$l/min (equivalent to a Reynolds number of $Re\approx 0.07$). Thirty-five images are captured at a low frame rate of 7 Hz with DX-format, 300 $\mu$s exposure, and 25600 ISO to maximize image resolution without binning.  See a diagram and photograph of the setup in figure \ref{fig:setup}.

This configuration implies a minimum number of spots behind each lenslet $N_u\approx$ 23.7 \citep{levoy2006light}, which is calculated as,
\begin{equation}
  N_u = \frac{p_l}{R_{obj}}, R_{obj} = \frac{0.47\lambda}{NA}M
\label{eq:Nu}
\end{equation}
where $\lambda$ is the emission wavelength of fluorescent particles, $NA$ is the numerical aperture, $M$ is the magnification of the objective lens, and $R_{obj}$ is the Sparrow limit for the smallest spacing between two distinguishable spots in the specimen \citep{inoue2013video}. The number of pixels in one lenslet,  $p_l/p\approx 28.7$, is larger than $N_u$ to ensure the pixel density does not limit the angular resolution. $N_u$ can also be regarded as the number of slices in the depth direction, leading to a depth resolution $D_{tot_2}$ $\approx$ 28.8 $\mu$m and a total reconstruction depth $D_V\approx 680$ $\mu$m \citep{levoy2006light}, which are computed as
\begin{equation}
  D_{tot_2} \approx \frac{(2 + N_u)\lambda n_m}{2NA^2}, D_V \approx N_uD_{tot_2}
\label{eq:DtotAndDV}
\end{equation}
where $n_m$ is the refractive index of the medium. 
To simplify the problem and to avoid under-sampling, the depth length of the voxel for image volume is set to $\Delta z_r$ $\approx$ $D_{tot_2}M^2/3$ $\approx$ 4 mm. Furthermore, $\Delta z_r$ ($\approx 4$ mm) also prevents reconstruction from the virtual image since it is larger than $f_m$. 
\begin{figure}[h]
       \centerline{\includegraphics[width=0.8\linewidth]{fig/figure6.jpg}}
       \caption{(a) General schematic of the microchannel and experimental setup; (b) photograph of the pre-calibrated MLA cage system by placing the MLA into intermediate image plane between 1:1 DSLR camera and microscope.}
       \label{fig:setup}
\end{figure}









\section*{Result and Discussion}
The  LF 3D$-\mu$PIV is compared with the theoretical fully-developed laminar solution for the flow rate of 0.85 $\mu$l/min.
Figure \ref{fig:result}a illustrates the velocity profile colored with velocity magnitude. A comparison of the velocity profiles along the major diagonals are shown in figure \ref{fig:result}b,c. The maximum velocity magnitude from the experiment matches well with the theoretical prediction. However, some departures are attributed to the image plane being too close to the MLA plane. Shorter $f_m$ offers more accurate reconstruction around the MLA plane but sacrifices depth of view. Furthermore, high particle velocity in the central region also destabilizes the linking algorithm, which may also affect the velocity field estimation. Finally, the relative measurement error is calculated as
\begin{equation}
  e(y,z) =  \frac{\left| u_{exp}(y,z) - u_{theo}(y,z) \right|}{u_{theo}(y,z)} \times 100
\label{eq:Uncertainty}
\end{equation}
where $u_{exp}(y,z)$, $u_{theo}(y,z)$ are velocity profile based on experiment and theoretical prediction.
As a result, the relative error is around 5$\%$ in the corner region and 12 $\%$ at the center. Overall, the bulk flow rate is estimated as 0.834 $\mu$m/min, close to the experimental flow rate of 0.85$\mu$m/min.
\begin{figure}[h]
       \centerline{\includegraphics[width = 0.6\linewidth]{fig/figure8.jpg}} 
       \caption{(a) A view of the velocity distribution; (b) comparison of the measured velocity profile along the $y-z$ diagonal direction with theoretical profile; (c) same as b) but in the other diagonal.}
     \label{fig:result}
\end{figure}








%what about: Achieving Robust 3D Particle Tracking at the Centimeter Scale
%better: Towards cm-scale Robust 3D Particle Tracking 
\chapter{Achieving Robust 3D Particle Tracking at the Centimeter Scale}\label{chapter:cm}
Flow characterization at mm and cm scales using multi-camera-based methods in Eulerian and Lagrangian frames of reference is typically challenging due to the shallow depth of field from macro lenses and calibration procedures. Our objective is to enhance the existing rapid 3D particle tracking system for cm-scale volume application, utilizing a microlens array (MLA) in the intermediate plane between the macro lens and camera sensor, as illustrated in \autoref{fig:ZoomLens_Setup}. This approach enables capturing light field information using a single camera cage system, eliminating the need for stereo calibration. 

To reconstruct 3D particle position, we employ a fast ray intersection and cloud point classification method, which is accelerated using GPU processing. The method discretizes image space uniformly, maps it to non-uniform objective space based on macro lens properties, and processes each image in under one second using atomic operations within the GPU's onboard memory.
We utilize the Lagrangian particle tracking Crocker-Grier method for particle linking to characterize the time-resolved flow field. With these enhancements, we expect to overcome the challenges typically faced in multi-camera-based flow characterization at shallow depths of field, enabling more accurate and efficient particle tracking in cm-scale volumes.
\begin{figure}[h]
  \centerline{\includegraphics[width = 0.7\linewidth]{fig/ZoomLens_Setup.png}} 
  \caption{Light field system with 25mm $f$/2.8 2.5 $\times$ macro lens. 
  }
\label{fig:ZoomLens_Setup}
\end{figure}









\section*{1. Towards Real-time Tracking through Parallelized Ray Counting}

The attainment of near-instantaneous tracking results is a prerequisite for effectively implementing cm-scale sparse particle tracking across numerous applications, including detecting micro-organism activities and facilitating controllable flow structure interactions. In this regard, we are implementing modern processors' computational power to pursue the realization of real-time tracking capabilities.

\autoref{fig:GPU} depicts how raw LF images may be reorganized into a 4D matrix such that each row corresponds to a single pixel that can be processed by a single thread. This approach allows for every pixel in each image to be processed concurrently, resulting in reduced reconstruction time. To mitigate memory transfer latency, the target volume is discretized into $400 \times 200 \times 150$ voxels. The 3D reconstruction time for a single frame is compared across different methods, including SIMD, OpenMP, MPI, and CUDA, as shown in \autoref{fig:running_time}. GPU-based parallel computing is as effective as the MPI method while being more accessible, as it can be executed on a desktop as opposed to multi-nodes on a supercluster.
\begin{figure}[h]
  \centerline{\includegraphics[width = 0.9\linewidth]{fig/GPU.png}} 
  \caption{Schematics of parallelized computing with GPU.}
\label{fig:GPU}
\end{figure}

\begin{figure}[h]
  \centerline{\includegraphics[width = 0.6\linewidth]{python/running_time.png}} 
  \caption{Computation time for SIMD, OpenMP, MPI, CUDA method.}
\label{fig:running_time}
\end{figure}
The objective is to merge the parallelized reconstruction step with an upgraded classification step and tree searching step (\autoref{fig:flowchart}) to attain real-time tracking capabilities.

\begin{figure}[h]
  \centerline{\includegraphics[width = 0.7\linewidth]{fig/CompareOP.png}} 
  \caption{(a) Orthographic calibration; (b) Zoom lens calibration. }
\label{fig:compareOP}
\end{figure}


\section*{2. Perspective View Correction through Lens Distortion Adjustment}
Compared to an orthographic imaging system, such as microscopy, a perspective zoom lens is more susceptible to lens distortion due to its shorter effective focal length (EFL). As shown in \autoref{fig:compareOP}, significant distortion occurs when approaching the edges of zoom lens images, making it impossible to utilize the circle's center to represent the lenslet center. While the aperture size can be adjusted in microscopical systems to calibrate for distortion, the same approach cannot be employed in zoom lens systems, as it would introduce pixel-level displacement between optical elements and compromise calibration accuracy (\autoref{fig:distortion}). In this segment, we will develop an image processing-based calibration technique to correct distorted subimages and accurately identify lenslet centers.


\begin{figure*}[h]
  \centerline{\includegraphics[width=0.9\linewidth]{fig/figure7.jpg}} 
  \caption{(a) An MLA grid overlaid on the image-based on the center alignment (grid points are center of each lenslet); (b) calibration images with small condenser aperture size, leading to precise micro image identification everywhere on the image (red points indicate the center of each lenslet).}
\label{fig:distortion}
\end{figure*}



\section*{3. Simulation}
Undoubtedly, a physically-based simulation is crucial to comprehend the uncertainties associated with perspective view systems. To facilitate our forthcoming experiments, we have fabricated a 12-element $f/2.8$, 2.5 zoom-ratio macro lens (\autoref{fig:MLD}) \citep{smith2005modern}, which is based on the $f/2.8$, 2.5 zoom-ratio Venus Laowa macro lens. To assess the uncertainty of the short focal macro lens, we have conducted 2D Zemax and 3D Blender simulations.

\begin{figure*}[h]
  \centerline{\includegraphics[width=0.7\linewidth]{fig/modern_lens_design.png}} 
  \caption{12-elements short focal macro lens.}
\label{fig:MLD}
\end{figure*}



\section*{4. Experiment}
Our current approach employs this methodology to investigate the fluid flow induced by boundary oscillations. To accomplish this, we utilize resonance speakers to vibrate a boundary at varying frequencies while simultaneously capturing the particle motion within a 7 mm $\times$ 7 mm $\times$ 7 mm volume using our high-frequency light field (LF) camera system. The resulting data will facilitate a comprehensive analysis of the fluid dynamics induced by the boundary vibrations.





\bibliography{thesisbib}
%\bibliography{reference}
\bibliographystyle{plainnat}



\end{document}
\endinput
%%
%% End of file `thesis-ex.tex'.
